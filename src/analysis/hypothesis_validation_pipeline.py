import os
import json
import pandas as pd
import sys
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.api import get_openai_response
from google.cloud import bigquery
from src.config import (
    PROJECT_ID,
    LLM_RESPONSES_DIR,
    SCHEMA_TXT_FILE,
    DATA_EXPLORATION_FILE,
    MAX_RETRIES,
    QUANTITATIVE_REPORT_FILE,
    HYPOTHESES_FILE,
    HYPOTHESIS_REPORTS_DIR
)

# â€•â€•â€• ãƒ­ã‚°è¨­å®š â€•â€•â€•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hypothesis_validation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# â€•â€•â€• è¨­å®š â€•â€•â€•
SQL_RETRY_LIMIT = 5  # SQLä¿®æ­£ã®ä¸Šé™å›æ•°
ANALYSIS_RETRY_LIMIT = 3  # åˆ†æçµæœä¿®æ­£ã®ä¸Šé™å›æ•°
MIN_REQUIRED_ROWS = 1  # æœ€ä½é™å¿…è¦ãªçµæœè¡Œæ•°

os.environ["GOOGLE_CLOUD_PROJECT"] = PROJECT_ID
client = bigquery.Client(project=PROJECT_ID)

class HypothesisValidationPipeline:
    """ä»®èª¬æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self):
        self.schema_text = self.load_file(SCHEMA_TXT_FILE)
        self.data_exploration = self.load_file(DATA_EXPLORATION_FILE)
        self.hypotheses = self.load_hypotheses()
        
    def load_file(self, filepath: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                return f.read().strip()
        except FileNotFoundError:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
            return ""
    
    def load_hypotheses(self) -> List[Dict]:
        """ä»®èª¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(HYPOTHESES_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"ä»®èª¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {HYPOTHESES_FILE}")
            return []
    
    def generate_sql_with_context(self, hypothesis: Dict, error_message: str = "") -> str:
        """ä»®èª¬ã«åŸºã¥ã„ã¦SQLã‚’ç”Ÿæˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è€ƒæ…®ï¼‰"""
        
        retry_context = ""
        if error_message:
            retry_context = f"""
            
**å‰å›ã®SQLå®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:**
```
{error_message}
```

ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ã€æ­£ã—ãå‹•ä½œã™ã‚‹SQLã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""
        
        prompt = f"""
ã‚ãªãŸã¯BigQueryã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€ä»®èª¬ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

## ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ å®šç¾©
```
{self.schema_text}
```

## å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹
```
{self.data_exploration}
```

## æ¤œè¨¼ã™ã‚‹ä»®èª¬
```json
{json.dumps(hypothesis, ensure_ascii=False, indent=2)}
```

{retry_context}

## è¦ä»¶
1. å¿…ãšå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆåã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
2. WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131' ã‚’å«ã‚ã¦ãã ã•ã„
3. çµæœã«ã¯ä»¥ä¸‹ã®ã‚«ãƒ©ãƒ ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
   - step3_users: ã‚¹ãƒ†ãƒƒãƒ—3ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
   - step4_users: ã‚¹ãƒ†ãƒƒãƒ—4ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°  
   - transition_rate: é·ç§»ç‡ (step4_users / step3_users)
4. ãƒ‡ãƒã‚¤ã‚¹ã‚«ãƒ†ã‚´ãƒªã‚„æµå…¥å…ƒãªã©ã€ä»®èª¬ã®æ¡ä»¶ã«åˆã‚ã›ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã—ã¦ãã ã•ã„

SQLã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ã¯ä¸è¦ï¼‰ï¼š
"""
        
        return get_openai_response(prompt).strip()
    
    def execute_sql_with_retry(self, hypothesis: Dict) -> Tuple[Optional[pd.DataFrame], str]:
        """SQLã‚’å®Ÿè¡Œï¼ˆå†å¸°çš„ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
        hyp_id = hypothesis.get("id", "UNKNOWN")
        logger.info(f"ğŸš€ {hyp_id}: SQLç”Ÿæˆã¨å®Ÿè¡Œã‚’é–‹å§‹")
        
        error_message = ""
        
        for attempt in range(1, SQL_RETRY_LIMIT + 1):
            logger.info(f"  è©¦è¡Œ {attempt}/{SQL_RETRY_LIMIT}")
            
            # SQLç”Ÿæˆ
            try:
                sql = self.generate_sql_with_context(hypothesis, error_message)
                
                # SQLã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
                if sql.startswith("```sql"):
                    sql = sql[6:]  # ```sql ã‚’é™¤å»
                if sql.endswith("```"):
                    sql = sql[:-3]  # ``` ã‚’é™¤å»
                sql = sql.strip()
                
                # SQLã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if not sql or len(sql.strip()) < 20:
                    raise ValueError("ç”Ÿæˆã•ã‚ŒãŸSQLãŒçŸ­ã™ãã¾ã™")
                
                # SQLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                sql_file = os.path.join(LLM_RESPONSES_DIR, f"{hyp_id}.json")
                with open(sql_file, "w", encoding="utf-8") as f:
                    json.dump({"sql": sql, "attempt": attempt, "timestamp": datetime.now().isoformat()}, f, ensure_ascii=False, indent=2)
                
                logger.info(f"  SQLç”Ÿæˆå®Œäº†: {len(sql)}æ–‡å­—")
                
            except Exception as e:
                error_message = f"SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
                logger.warning(f"  {error_message}")
                continue
            
            # SQLå®Ÿè¡Œ
            try:
                logger.info(f"  SQLå®Ÿè¡Œé–‹å§‹...")
                job = client.query(sql)
                result_df = job.to_dataframe()
                
                # çµæœã®æ¤œè¨¼
                if result_df.empty:
                    raise ValueError("ã‚¯ã‚¨ãƒªçµæœãŒç©ºã§ã™")
                
                if len(result_df) < MIN_REQUIRED_ROWS:
                    raise ValueError(f"çµæœè¡Œæ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {len(result_df)}è¡Œ")
                
                # å¿…è¦ãªã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
                required_columns = ['step3_users', 'step4_users', 'transition_rate']
                missing_columns = [col for col in required_columns if col not in result_df.columns]
                
                if missing_columns:
                    raise ValueError(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_columns}")
                
                logger.info(f"  âœ… SQLå®Ÿè¡ŒæˆåŠŸ: {len(result_df)}è¡Œã®çµæœ")
                return result_df, ""
                
            except Exception as e:
                error_message = f"SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
                logger.warning(f"  âš ï¸ {error_message}")
                
                if attempt == SQL_RETRY_LIMIT:
                    logger.error(f"  âŒ {hyp_id}: {SQL_RETRY_LIMIT}å›è©¦è¡Œå¾Œã‚‚å¤±æ•—")
                    return None, error_message
        
        return None, error_message
    
    def generate_analysis_report(self, hypothesis: Dict, result_df: pd.DataFrame, previous_error: str = "") -> str:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        retry_context = ""
        if previous_error:
            retry_context = f"""
            
**å‰å›ã®åˆ†æã§å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ:**
```
{previous_error}
```

ä¸Šè¨˜ã®å•é¡Œã‚’è§£æ±ºã—ã¦ã€ã‚ˆã‚Šè©³ç´°ã§æœ‰ç”¨ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""
        
        # çµæœã‚’æ–‡å­—åˆ—åŒ–
        result_summary = result_df.describe().to_string()
        result_data = result_df.to_string(index=False)
        
        prompt = f"""
ä»¥ä¸‹ã®ä»®èª¬æ¤œè¨¼çµæœã‚’åˆ†æã—ã¦ã€æ—¥æœ¬èªã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## æ¤œè¨¼ã—ãŸä»®èª¬
```json
{json.dumps(hypothesis, ensure_ascii=False, indent=2)}
```

## åˆ†æçµæœãƒ‡ãƒ¼ã‚¿
```
{result_data}
```

## çµ±è¨ˆã‚µãƒãƒªãƒ¼
```
{result_summary}
```

{retry_context}

## ãƒ¬ãƒãƒ¼ãƒˆè¦ä»¶
1. ä»®èª¬ã®æ¤œè¨¼çµæœã‚’æ˜ç¢ºã«è¿°ã¹ã¦ãã ã•ã„
2. æ•°å€¤çš„ãªæ ¹æ‹ ã‚’ç¤ºã—ã¦ãã ã•ã„
3. ãƒ“ã‚¸ãƒã‚¹ä¸Šã®ç¤ºå”†ã‚’å«ã‚ã¦ãã ã•ã„
4. æ”¹å–„ææ¡ˆãŒã‚ã‚Œã°å«ã‚ã¦ãã ã•ã„
5. å°‚é–€çš„ã™ããšã€ãƒ“ã‚¸ãƒã‚¹æ‹…å½“è€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ãæ›¸ã„ã¦ãã ã•ã„

æ—¥æœ¬èªã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
"""
        
        return get_openai_response(prompt).strip()
    
    def validate_analysis_quality(self, report: str) -> Tuple[bool, str]:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å“è³ªã‚’æ¤œè¨¼"""
        
        if len(report) < 100:
            return False, "ãƒ¬ãƒãƒ¼ãƒˆãŒçŸ­ã™ãã¾ã™"
        
        if "ä»®èª¬" not in report:
            return False, "ä»®èª¬ã«é–¢ã™ã‚‹è¨€åŠãŒã‚ã‚Šã¾ã›ã‚“"
        
        if not any(char.isdigit() for char in report):
            return False, "æ•°å€¤çš„æ ¹æ‹ ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
        
        return True, ""
    
    def generate_analysis_with_retry(self, hypothesis: Dict, result_df: pd.DataFrame) -> str:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå†å¸°çš„ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
        hyp_id = hypothesis.get("id", "UNKNOWN")
        logger.info(f"ğŸ“Š {hyp_id}: åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        
        previous_error = ""
        
        for attempt in range(1, ANALYSIS_RETRY_LIMIT + 1):
            logger.info(f"  åˆ†æè©¦è¡Œ {attempt}/{ANALYSIS_RETRY_LIMIT}")
            
            try:
                # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                report = self.generate_analysis_report(hypothesis, result_df, previous_error)
                
                # å“è³ªæ¤œè¨¼
                is_valid, error_msg = self.validate_analysis_quality(report)
                
                if is_valid:
                    logger.info(f"  âœ… åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”ŸæˆæˆåŠŸ: {len(report)}æ–‡å­—")
                    return report
                else:
                    previous_error = error_msg
                    logger.warning(f"  âš ï¸ å“è³ªå•é¡Œ: {error_msg}")
                    
            except Exception as e:
                previous_error = f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
                logger.warning(f"  âš ï¸ {previous_error}")
        
        # æœ€çµ‚çš„ã«å¤±æ•—ã—ãŸå ´åˆã¯åŸºæœ¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã™
        logger.warning(f"  ğŸ”„ åŸºæœ¬ãƒ¬ãƒãƒ¼ãƒˆã§ä»£æ›¿")
        return f"""
# {hyp_id} æ¤œè¨¼çµæœ

## ä»®èª¬
{hypothesis.get('summary', 'ä¸æ˜')}

## çµæœ
åˆ†æã¯å®Œäº†ã—ã¾ã—ãŸãŒã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚
ãƒ‡ãƒ¼ã‚¿ã¯æ­£å¸¸ã«å–å¾—ã•ã‚Œã¦ãŠã‚Šã€çµæœã¯ {len(result_df)} è¡Œã§ã™ã€‚

æ‰‹å‹•ã§ã®è©³ç´°åˆ†æã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚
"""
    
    def process_single_hypothesis(self, hypothesis: Dict) -> bool:
        """å˜ä¸€ä»®èª¬ã®å‡¦ç†"""
        hyp_id = hypothesis.get("id", "UNKNOWN")
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¯ å‡¦ç†é–‹å§‹: {hyp_id}")
        logger.info(f"{'='*60}")
        
        try:
            # SQLå®Ÿè¡Œï¼ˆå†å¸°çš„ãƒªãƒˆãƒ©ã‚¤ï¼‰
            result_df, sql_error = self.execute_sql_with_retry(hypothesis)
            
            if result_df is None:
                logger.error(f"âŒ {hyp_id}: SQLå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ - {sql_error}")
                return False
            
            # åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå†å¸°çš„ãƒªãƒˆãƒ©ã‚¤ï¼‰
            report = self.generate_analysis_with_retry(hypothesis, result_df)
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            report_file = os.path.join(HYPOTHESIS_REPORTS_DIR, f"{hyp_id}_report.txt")
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            
            logger.info(f"âœ… {hyp_id}: å‡¦ç†å®Œäº† - {report_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {hyp_id}: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {str(e)}")
            return False
    
    def generate_final_report(self, successful_hypotheses: List[str]) -> None:
        """æœ€çµ‚çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        logger.info("\nğŸ“‹ æœ€çµ‚çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        
        if not successful_hypotheses:
            logger.warning("æˆåŠŸã—ãŸä»®èª¬ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # å„ä»®èª¬ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        all_reports = []
        for hyp_id in successful_hypotheses:
            report_file = os.path.join(HYPOTHESIS_REPORTS_DIR, f"{hyp_id}_report.txt")
            try:
                with open(report_file, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    all_reports.append(f"## {hyp_id}\n\n{content}")
            except FileNotFoundError:
                logger.warning(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {report_file}")
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        final_report = f"""# GA4 ä»®èª¬æ¤œè¨¼ çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ¤œè¨¼æˆåŠŸ**: {len(successful_hypotheses)}ä»¶ / {len(self.hypotheses)}ä»¶

## æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼

{chr(10).join(all_reports)}

## ç·åˆæ‰€è¦‹

æœ¬æ¤œè¨¼ã«ã‚ˆã‚Šã€{len(successful_hypotheses)}ä»¶ã®ä»®èª¬ã«ã¤ã„ã¦å®šé‡çš„ãªæ¤œè¨¼ã‚’å®Œäº†ã—ã¾ã—ãŸã€‚
å„ä»®èª¬ã®è©³ç´°ãªåˆ†æçµæœã¯ä¸Šè¨˜ã‚’ã”å‚ç…§ãã ã•ã„ã€‚

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(QUANTITATIVE_REPORT_FILE, "w", encoding="utf-8") as f:
            f.write(final_report)
        
        logger.info(f"âœ… çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {QUANTITATIVE_REPORT_FILE}")
    
    def run_pipeline(self) -> None:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        start_time = datetime.now()
        logger.info(f"\nğŸš€ ä»®èª¬æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æ¤œè¨¼å¯¾è±¡: {len(self.hypotheses)}ä»¶ã®ä»®èª¬")
        
        successful_hypotheses = []
        
        for i, hypothesis in enumerate(self.hypotheses, 1):
            logger.info(f"\né€²æ—: {i}/{len(self.hypotheses)}")
            
            success = self.process_single_hypothesis(hypothesis)
            if success:
                successful_hypotheses.append(hypothesis.get("id", f"H{i:03d}"))
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_final_report(successful_hypotheses)
        
        # çµæœã‚µãƒãƒªãƒ¼
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ‰ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
        logger.info(f"{'='*60}")
        logger.info(f"å®Ÿè¡Œæ™‚é–“: {duration}")
        logger.info(f"æˆåŠŸ: {len(successful_hypotheses)}/{len(self.hypotheses)} ä»¶")
        logger.info(f"æˆåŠŸç‡: {len(successful_hypotheses)/len(self.hypotheses)*100:.1f}%")
        logger.info(f"çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ: {QUANTITATIVE_REPORT_FILE}")
        
        if successful_hypotheses:
            logger.info(f"æˆåŠŸã—ãŸä»®èª¬: {', '.join(successful_hypotheses)}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        pipeline = HypothesisValidationPipeline()
        pipeline.run_pipeline()
    except Exception as e:
        logger.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()