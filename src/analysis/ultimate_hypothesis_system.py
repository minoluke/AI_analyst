"""
ç©¶æ¥µã®ä»®èª¬ç”Ÿæˆãƒ»æ”¹è‰¯ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
- æ®µéšŽçš„æ”¹è‰¯ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆAI-Scientist-v2ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
- ç³»çµ±çš„å®Ÿé¨“å®Ÿè¡Œï¼ˆD2D_Data2Dashboardã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
"""

import json
import asyncio
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
import sys
import os
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.api import get_openai_response
from src.config import PROJECT_ID, DATASET_ID, SCHEMA_TXT_FILE, DATA_EXPLORATION_FILE
from src.config_analysis import get_analysis_config
from google.cloud import bigquery

@dataclass
class ExperimentResult:
    """å®Ÿé¨“çµæžœã®æ§‹é€ """
    experiment_id: str
    hypothesis_id: str
    control_group_size: int
    treatment_group_size: int
    control_metric: float
    treatment_metric: float
    effect_size: float
    statistical_significance: bool
    confidence_interval: tuple

@dataclass
class HypothesisEvolution:
    """ä»®èª¬ã®é€²åŒ–ãƒ—ãƒ­ã‚»ã‚¹"""
    original: Dict
    evaluations: List[Dict]
    refinements: List[Dict]
    final_version: Dict
    validation_score: float

class UltimateHypothesisSystem:
    """çµ±åˆã•ã‚ŒãŸä»®èª¬ç”Ÿæˆãƒ»æ”¹è‰¯ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        os.environ["GOOGLE_CLOUD_PROJECT"] = PROJECT_ID
        self.client = bigquery.Client(project=PROJECT_ID)
        self.config = get_analysis_config()
        self.refinement_rounds = self.config.processing.refinement_rounds
        self.session_start = datetime.now()
        
    # ==========================================
    # Phase 1: æ®µéšŽçš„æ”¹è‰¯ãƒ—ãƒ­ã‚»ã‚¹
    # ==========================================
    
    def generate_initial_hypotheses(self, schema_text: str, data_exploration: str) -> List[Dict]:
        """ðŸŽ¯ Phase 1.1: åˆæœŸä»®èª¬ç”Ÿæˆ"""
        print("ðŸŽ¯ Phase 1.1: åˆæœŸä»®èª¬ç”Ÿæˆä¸­...")
        
        prompt = f"""
GA4 eã‚³ãƒžãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ãƒ“ã‚¸ãƒã‚¹æ”¹å–„ã«ç›´çµã™ã‚‹3ã¤ã®åˆæœŸä»®èª¬ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘
â–  GA4ã‚¹ã‚­ãƒ¼ãƒž:
{schema_text[:2000]}...

â–  å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹:
{data_exploration[:2000]}...

ã€è¦ä»¶ã€‘
1. æ¤œè¨¼å¯èƒ½æ€§: BigQueryã§æ¸¬å®šã§ãã‚‹
2. æ¯”è¼ƒåŸºæº–ã®æ˜Žç¢ºæ€§: ä½•ã¨ä½•ã‚’æ¯”è¼ƒã™ã‚‹ã‹æ˜Žè¨˜
3. ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤: æ„æ€æ±ºå®šã«ç›´çµã™ã‚‹æ´žå¯Ÿ

ã€å‡ºåŠ›å½¢å¼ã€‘
```json
[
  {{
    "id": "H001",
    "hypothesis": "å…·ä½“çš„ã§æ¤œè¨¼å¯èƒ½ãªä»®èª¬ï¼ˆæ¯”è¼ƒå¯¾è±¡æ˜Žè¨˜ï¼‰",
    "rationale": "ä»®èª¬ã‚’ç«‹ã¦ãŸæ ¹æ‹ ãƒ»èƒŒæ™¯",
    "business_question": "è§£æ±ºã—ãŸã„ãƒ“ã‚¸ãƒã‚¹èª²é¡Œ",
    "potential_issues": ["æƒ³å®šã•ã‚Œã‚‹åˆ†æžä¸Šã®å•é¡Œç‚¹"],
    "success_criteria": "ä»®èª¬ãŒæˆç«‹ã™ã‚‹æ¡ä»¶"
  }}
]
```
"""
        
        response = get_openai_response(prompt)
        hypotheses = self._extract_json_list(response)
        print(f"âœ… åˆæœŸä»®èª¬ {len(hypotheses)}ä»¶ ç”Ÿæˆå®Œäº†")
        return hypotheses
    
    def critical_evaluation(self, hypotheses: List[Dict]) -> List[Dict]:
        """ðŸ” Phase 1.2: æ‰¹åˆ¤çš„è©•ä¾¡"""
        print("ðŸ” Phase 1.2: æ‰¹åˆ¤çš„è©•ä¾¡ä¸­...")
        
        evaluation_prompt = f"""
ä»¥ä¸‹ã®ä»®èª¬ã‚’åŽ³æ ¼ã«è©•ä¾¡ã—ã€æ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚

ã€ä»®èª¬ã€‘
{json.dumps(hypotheses, ensure_ascii=False, indent=2)}

ã€è©•ä¾¡åŸºæº–ã€‘
1. æ¤œè¨¼å¯èƒ½æ€§ï¼ˆBigQueryã§å®Ÿéš›ã«æ¸¬å®šã§ãã‚‹ã‹ï¼‰
2. æ¯”è¼ƒåŸºæº–ã®æ˜Žç¢ºæ€§ï¼ˆAã¨Bã®æ¯”è¼ƒãŒæ˜Žç¢ºã‹ï¼‰
3. ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ï¼ˆçµæžœãŒå…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ç¹‹ãŒã‚‹ã‹ï¼‰
4. çµ±è¨ˆçš„å¦¥å½“æ€§ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ãƒ»ãƒã‚¤ã‚¢ã‚¹ãƒ»æ¤œå‡ºåŠ›ï¼‰
5. å®Ÿè¡Œå¯èƒ½æ€§ï¼ˆç¾å®Ÿçš„ãªæ™‚é–“ãƒ»ãƒªã‚½ãƒ¼ã‚¹å†…ã§å®Ÿè¡Œå¯èƒ½ã‹ï¼‰
6. ãƒ‡ãƒ¼ã‚¿åˆ¶ç´„ï¼ˆåˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿ã§ã®åˆ¶é™ï¼‰

ã€å‡ºåŠ›å½¢å¼ã€‘
```json
[
  {{
    "hypothesis_id": "H001",
    "overall_score": 7.5,
    "strengths": ["å…·ä½“çš„ãªå¼·ã¿"],
    "critical_weaknesses": ["é‡å¤§ãªå¼±ç‚¹"],
    "improvement_suggestions": [
      {{
        "category": "æ¤œè¨¼å¯èƒ½æ€§",
        "suggestion": "å…·ä½“çš„æ”¹å–„æ¡ˆ",
        "priority": "high/medium/low"
      }}
    ],
    "feasibility_concerns": ["å®Ÿè¡Œä¸Šã®æ‡¸å¿µç‚¹"],
    "recommendation": "accept/revise/reject"
  }}
]
```
"""
        
        response = get_openai_response(evaluation_prompt)
        evaluations = self._extract_json_list(response)
        print(f"âœ… è©•ä¾¡å®Œäº†: {len(evaluations)}ä»¶")
        return evaluations
    
    def refine_hypotheses(self, original_hypotheses: List[Dict], evaluations: List[Dict]) -> List[Dict]:
        """âš¡ Phase 1.3: æ”¹è‰¯ç‰ˆç”Ÿæˆ"""
        print("âš¡ Phase 1.3: ä»®èª¬æ”¹è‰¯ä¸­...")
        
        refinement_prompt = f"""
è©•ä¾¡çµæžœã‚’åŸºã«ä»®èª¬ã‚’æ ¹æœ¬çš„ã«æ”¹è‰¯ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®ä»®èª¬ã€‘
{json.dumps(original_hypotheses, ensure_ascii=False, indent=2)}

ã€è©•ä¾¡çµæžœã€‘
{json.dumps(evaluations, ensure_ascii=False, indent=2)}

ã€æ”¹è‰¯è¦æ±‚ã€‘
- é‡å¤§ãªå¼±ç‚¹ã‚’æ ¹æœ¬çš„ã«ä¿®æ­£
- æ”¹å–„ææ¡ˆã‚’å…¨ã¦åæ˜ 
- ã‚ˆã‚Šå…·ä½“çš„ã§æ¸¬å®šå¯èƒ½ã«
- æ¯”è¼ƒåŸºæº–ã‚’æ•°å€¤ã§æ˜Žç¢ºåŒ–
- ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã‚’å…·ä½“åŒ–

ã€å‡ºåŠ›å½¢å¼ã€‘
```json
[
  {{
    "id": "H001_refined",
    "original_hypothesis": "å…ƒã®ä»®èª¬",
    "refined_hypothesis": "æ ¹æœ¬çš„ã«æ”¹è‰¯ã•ã‚ŒãŸä»®èª¬",
    "improvements_made": [
      {{
        "area": "æ”¹è‰¯é ˜åŸŸ",
        "before": "æ”¹è‰¯å‰ã®çŠ¶æ…‹",
        "after": "æ”¹è‰¯å¾Œã®çŠ¶æ…‹"
      }}
    ],
    "comparison_baseline": "æ˜Žç¢ºãªæ¯”è¼ƒåŸºæº–ï¼ˆæ•°å€¤å«ã‚€ï¼‰",
    "verification_method": "å…·ä½“çš„æ¤œè¨¼æ‰‹æ³•",
    "expected_business_impact": "æœŸå¾…ã•ã‚Œã‚‹ãƒ“ã‚¸ãƒã‚¹åŠ¹æžœ"
  }}
]
```
"""
        
        response = get_openai_response(refinement_prompt)
        refined = self._extract_json_list(response)
        print(f"âœ… æ”¹è‰¯å®Œäº†: {len(refined)}ä»¶")
        return refined
    
    def final_validation(self, refined_hypotheses: List[Dict]) -> List[Dict]:
        """âœ… Phase 1.4: æœ€çµ‚æ¤œè¨¼"""
        print("âœ… Phase 1.4: æœ€çµ‚æ¤œè¨¼ä¸­...")
        
        validation_prompt = f"""
æ”¹è‰¯ã•ã‚ŒãŸä»®èª¬ã®æœ€çµ‚å“è³ªæ¤œè¨¼ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ã€æ”¹è‰¯ä»®èª¬ã€‘
{json.dumps(refined_hypotheses, ensure_ascii=False, indent=2)}

ã€æ¤œè¨¼é …ç›®ã€‘
1. å®Œå…¨æ€§: å…¨ã¦ã®è¦ç´ ãŒé©åˆ‡ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹
2. æ˜Žç¢ºæ€§: æ›–æ˜§ã•ã‚„è§£é‡ˆã®ä½™åœ°ãŒãªã„ã‹
3. å®Ÿè¡Œå¯èƒ½æ€§: ç¾å®Ÿçš„ã«æ¤œè¨¼å¯èƒ½ã‹
4. ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤: æ„æ€æ±ºå®šã«æ˜Žç¢ºã«è²¢çŒ®ã™ã‚‹ã‹
5. çµ±è¨ˆçš„åŽ³å¯†æ€§: é©åˆ‡ãªæ¤œè¨¼æ–¹æ³•ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹

ã€å‡ºåŠ›å½¢å¼ã€‘
```json
[
  {{
    "id": "H001_final",
    "hypothesis": "æœ€çµ‚ç¢ºå®šä»®èª¬",
    "validation_status": "pass",
    "confidence_score": 8.5,
    "business_impact": "å…·ä½“çš„ãªãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤",
    "verification_plan": {{
      "primary_metric": "ä¸»è¦æ¸¬å®šæŒ‡æ¨™",
      "comparison_groups": "æ¯”è¼ƒå¯¾è±¡",
      "success_threshold": "æˆåŠŸåŸºæº–",
      "statistical_method": "çµ±è¨ˆæ‰‹æ³•"
    }},
    "quality_assurance": {{
      "completeness": 9.0,
      "clarity": 8.5,
      "feasibility": 8.0,
      "business_value": 9.0
    }}
  }}
]
```
"""
        
        response = get_openai_response(validation_prompt)
        final_hypotheses = self._extract_json_list(response)
        print(f"âœ… æœ€çµ‚æ¤œè¨¼å®Œäº†: {len(final_hypotheses)}ä»¶")
        return final_hypotheses
    
    # ==========================================
    # Phase 2: ç³»çµ±çš„å®Ÿé¨“å®Ÿè¡Œ
    # ==========================================
    
    def design_experimental_groups(self, hypothesis: Dict) -> Dict:
        """ðŸ§ª Phase 2.1: å®Ÿé¨“ç¾¤ãƒ»å¯¾ç…§ç¾¤è¨­è¨ˆ"""
        print(f"ðŸ§ª Phase 2.1: {hypothesis['id']} å®Ÿé¨“è¨­è¨ˆä¸­...")
        
        design_prompt = f"""
ä»¥ä¸‹ã®ä»®èª¬ã«å¯¾ã—ã¦ã€åŽ³å¯†ãªA/Bãƒ†ã‚¹ãƒˆé¢¨ã®å®Ÿé¨“è¨­è¨ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ã€ä»®èª¬ã€‘
{hypothesis['hypothesis']}

ã€æ¤œè¨¼è¨ˆç”»ã€‘
{json.dumps(hypothesis.get('verification_plan', {}), ensure_ascii=False, indent=2)}

ã€è¨­è¨ˆè¦æ±‚ã€‘
1. å¯¾ç…§ç¾¤ãƒ»å®Ÿé¨“ç¾¤ã®å…·ä½“çš„å®šç¾©ï¼ˆSQL WHEREæ¡ä»¶å«ã‚€ï¼‰
2. ä¸»è¦æŒ‡æ¨™ã¨å‰¯æ¬¡æŒ‡æ¨™ã®è¨ˆç®—å¼
3. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®æŽ¨å®š
4. çµ±è¨ˆçš„æ¤œå‡ºåŠ›ã®è¨­å®š
5. æ½œåœ¨çš„äº¤çµ¡å› å­ã®ç‰¹å®š

ã€å‡ºåŠ›å½¢å¼ã€‘
```json
{{
  "control_group": {{
    "definition": "å¯¾ç…§ç¾¤ã®æ˜Žç¢ºãªå®šç¾©",
    "sql_filter": "WHERE device.category = 'desktop'",
    "expected_size": 50000,
    "characteristics": "ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´"
  }},
  "treatment_group": {{
    "definition": "å®Ÿé¨“ç¾¤ã®æ˜Žç¢ºãªå®šç¾©",
    "sql_filter": "WHERE device.category = 'mobile'", 
    "expected_size": 45000,
    "characteristics": "ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´"
  }},
  "primary_metric": {{
    "name": "è³¼å…¥è»¢æ›çŽ‡",
    "calculation": "è³¼å…¥ã‚¤ãƒ™ãƒ³ãƒˆæ•° / ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°",
    "sql_expression": "COUNTIF(event_name = 'purchase') / COUNT(DISTINCT user_pseudo_id)"
  }},
  "secondary_metrics": [
    {{
      "name": "å¹³å‡è³¼å…¥é¡",
      "sql_expression": "AVG(purchase_revenue_in_usd)"
    }}
  ],
  "statistical_approach": {{
    "test_type": "two-sample t-test",
    "alpha": 0.05,
    "power": 0.8,
    "minimum_detectable_effect": 0.1
  }},
  "confounding_factors": ["æ™‚é–“å¸¯", "åœ°åŸŸ", "ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³"],
  "data_quality_checks": ["æ¬ æå€¤ã®å‡¦ç†", "å¤–ã‚Œå€¤ã®æ¤œå‡º"]
}}
```
"""
        
        response = get_openai_response(design_prompt)
        design = self._extract_json_object(response)
        print(f"âœ… å®Ÿé¨“è¨­è¨ˆå®Œäº†: {design.get('primary_metric', {}).get('name', 'Unknown')}")
        return design
    
    def execute_exp01_basic_analysis(self, hypothesis: Dict, design: Dict) -> ExperimentResult:
        """ðŸ”¬ exp01: åŸºæœ¬è¨˜è¿°çµ±è¨ˆåˆ†æž"""
        print(f"ðŸ”¬ exp01_{hypothesis['id']}_basic_analysis å®Ÿè¡Œä¸­...")
        
        basic_sql = f"""
        WITH control_group AS (
          SELECT 
            user_pseudo_id,
            event_name,
            CASE WHEN event_name = 'purchase' THEN 1 ELSE 0 END as purchase_flag
          FROM `{PROJECT_ID}.{DATASET_ID}.events_*`
          WHERE {design['control_group']['sql_filter']}
            AND {self.config.get_sql_date_filter()}
        ),
        treatment_group AS (
          SELECT 
            user_pseudo_id,
            event_name,
            CASE WHEN event_name = 'purchase' THEN 1 ELSE 0 END as purchase_flag
          FROM `{PROJECT_ID}.{DATASET_ID}.events_*`
          WHERE {design['treatment_group']['sql_filter']}
            AND {self.config.get_sql_date_filter()}
        ),
        control_stats AS (
          SELECT 
            'control' as group_type,
            COUNT(DISTINCT user_pseudo_id) as total_users,
            COUNT(DISTINCT CASE WHEN purchase_flag = 1 THEN user_pseudo_id END) as purchasers,
            COUNT(DISTINCT CASE WHEN purchase_flag = 1 THEN user_pseudo_id END) / COUNT(DISTINCT user_pseudo_id) as conversion_rate
          FROM control_group
        ),
        treatment_stats AS (
          SELECT 
            'treatment' as group_type,
            COUNT(DISTINCT user_pseudo_id) as total_users,
            COUNT(DISTINCT CASE WHEN purchase_flag = 1 THEN user_pseudo_id END) as purchasers,
            COUNT(DISTINCT CASE WHEN purchase_flag = 1 THEN user_pseudo_id END) / COUNT(DISTINCT user_pseudo_id) as conversion_rate
          FROM treatment_group
        )
        SELECT * FROM control_stats
        UNION ALL
        SELECT * FROM treatment_stats
        """
        
        try:
            results = self.client.query(basic_sql).to_dataframe()
            
            if len(results) >= 2:
                control_data = results[results['group_type'] == 'control'].iloc[0]
                treatment_data = results[results['group_type'] == 'treatment'].iloc[0]
                
                control_rate = float(control_data['conversion_rate'])
                treatment_rate = float(treatment_data['conversion_rate'])
                effect_size = (treatment_rate - control_rate) / control_rate if control_rate > 0 else 0
                
                result = ExperimentResult(
                    experiment_id="exp01_basic",
                    hypothesis_id=hypothesis['id'],
                    control_group_size=int(control_data['total_users']),
                    treatment_group_size=int(treatment_data['total_users']),
                    control_metric=control_rate,
                    treatment_metric=treatment_rate,
                    effect_size=effect_size,
                    statistical_significance=False,
                    confidence_interval=(0, 0)
                )
                
                print(f"âœ… exp01 å®Œäº†: ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚º {effect_size:.4f}")
                return result
            else:
                print("âŒ exp01: ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                return None
                
        except Exception as e:
            print(f"âŒ exp01 å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def execute_exp02_comparative_analysis(self, hypothesis: Dict, design: Dict) -> ExperimentResult:
        """ðŸ“Š exp02: çµ±è¨ˆçš„æ¯”è¼ƒåˆ†æž"""
        print(f"ðŸ“Š exp02_{hypothesis['id']}_comparative_analysis å®Ÿè¡Œä¸­...")
        
        comparative_sql = f"""
        WITH control_group AS (
          SELECT 
            user_pseudo_id,
            CASE WHEN event_name = 'purchase' THEN 1 ELSE 0 END as purchase_flag
          FROM `{PROJECT_ID}.{DATASET_ID}.events_*`
          WHERE {design['control_group']['sql_filter']}
            AND {self.config.get_sql_date_filter()}
        ),
        treatment_group AS (
          SELECT 
            user_pseudo_id,
            CASE WHEN event_name = 'purchase' THEN 1 ELSE 0 END as purchase_flag
          FROM `{PROJECT_ID}.{DATASET_ID}.events_*`
          WHERE {design['treatment_group']['sql_filter']}
            AND {self.config.get_sql_date_filter()}
        ),
        control_agg AS (
          SELECT 
            'control' as group_type,
            COUNT(DISTINCT user_pseudo_id) as sample_size,
            AVG(purchase_flag) as conversion_rate,
            STDDEV(purchase_flag) as std_dev
          FROM control_group
        ),
        treatment_agg AS (
          SELECT 
            'treatment' as group_type,
            COUNT(DISTINCT user_pseudo_id) as sample_size,
            AVG(purchase_flag) as conversion_rate,
            STDDEV(purchase_flag) as std_dev
          FROM treatment_group
        )
        SELECT * FROM control_agg
        UNION ALL  
        SELECT * FROM treatment_agg
        """
        
        try:
            results = self.client.query(comparative_sql).to_dataframe()
            
            if len(results) >= 2:
                control_data = results[results['group_type'] == 'control'].iloc[0]
                treatment_data = results[results['group_type'] == 'treatment'].iloc[0]
                
                control_rate = float(control_data['conversion_rate'])
                treatment_rate = float(treatment_data['conversion_rate'])
                
                # ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚ºè¨ˆç®—
                effect_size = (treatment_rate - control_rate) / control_rate if control_rate > 0 else 0
                
                # çµ±è¨ˆçš„æœ‰æ„æ€§åˆ¤å®šï¼ˆè¨­å®šå€¤ã‚’ä½¿ç”¨ï¼‰
                significance = self.config.is_significant(effect_size)
                
                # ä¿¡é ¼åŒºé–“ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
                n_treatment = float(treatment_data['sample_size'])
                std_treatment = float(treatment_data['std_dev']) if treatment_data['std_dev'] else 0
                
                margin_of_error = 1.96 * (std_treatment / (n_treatment ** 0.5)) if n_treatment > 0 else 0
                ci_lower = treatment_rate - margin_of_error
                ci_upper = treatment_rate + margin_of_error
                
                result = ExperimentResult(
                    experiment_id="exp02_comparative",
                    hypothesis_id=hypothesis['id'],
                    control_group_size=int(control_data['sample_size']),
                    treatment_group_size=int(treatment_data['sample_size']),
                    control_metric=control_rate,
                    treatment_metric=treatment_rate,
                    effect_size=effect_size,
                    statistical_significance=significance,
                    confidence_interval=(ci_lower, ci_upper)
                )
                
                print(f"âœ… exp02 å®Œäº†: æœ‰æ„æ€§ {'â—‹' if significance else 'Ã—'}")
                return result
            else:
                print("âŒ exp02: ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                return None
                
        except Exception as e:
            print(f"âŒ exp02 å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def execute_exp03_advanced_segmentation(self, hypothesis: Dict, design: Dict) -> ExperimentResult:
        """ðŸŽ¯ exp03: é«˜åº¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æž"""
        print(f"ðŸŽ¯ exp03_{hypothesis['id']}_advanced_segmentation å®Ÿè¡Œä¸­...")
        
        segmentation_sql = f"""
        WITH all_data AS (
          SELECT 
            {self.config.schema.user_id_field},
            {self.config.schema.device_category_field} as device_type,
            {self.config.schema.geo_country_field} as country,
            CASE WHEN {design['control_group']['sql_filter'].replace('WHERE ', '')} THEN 'control'
                 WHEN {design['treatment_group']['sql_filter'].replace('WHERE ', '')} THEN 'treatment'
                 ELSE 'other' END as group_type,
            CASE WHEN {self.config.schema.purchase_event_condition} THEN 1 ELSE 0 END as purchase_flag
          FROM {self.config.get_full_table_reference()}
          WHERE {self.config.get_sql_date_filter()}
            AND ({design['control_group']['sql_filter'].replace('WHERE ', '')} 
                OR {design['treatment_group']['sql_filter'].replace('WHERE ', '')})
        ),
        segmented_results AS (
          SELECT 
            group_type,
            device_type,
            COUNT(DISTINCT {self.config.schema.user_id_field}) as sample_size,
            AVG(purchase_flag) as conversion_rate
          FROM all_data
          WHERE group_type IN ('control', 'treatment')
            AND device_type IS NOT NULL
          GROUP BY group_type, device_type
          HAVING COUNT(DISTINCT {self.config.schema.user_id_field}) > {self.config.statistical.min_sample_size}
        )
        SELECT * FROM segmented_results
        ORDER BY device_type, group_type
        """
        
        try:
            results = self.client.query(segmentation_sql).to_dataframe()
            
            if len(results) > 0:
                # ãƒ‡ãƒã‚¤ã‚¹åˆ¥åŠ¹æžœã‚µã‚¤ã‚ºè¨ˆç®—
                device_effects = []
                total_control = 0
                total_treatment = 0
                
                for device in results['device_type'].unique():
                    device_data = results[results['device_type'] == device]
                    
                    control_subset = device_data[device_data['group_type'] == 'control']
                    treatment_subset = device_data[device_data['group_type'] == 'treatment']
                    
                    if len(control_subset) > 0 and len(treatment_subset) > 0:
                        control_rate = float(control_subset.iloc[0]['conversion_rate'])
                        treatment_rate = float(treatment_subset.iloc[0]['conversion_rate'])
                        
                        if control_rate > 0:
                            device_effect = (treatment_rate - control_rate) / control_rate
                            device_effects.append(device_effect)
                
                # å…¨ä½“é›†è¨ˆ
                control_total = results[results['group_type'] == 'control']['sample_size'].sum()
                treatment_total = results[results['group_type'] == 'treatment']['sample_size'].sum()
                
                overall_effect = sum(device_effects) / len(device_effects) if device_effects else 0
                significance = self.config.is_significant(overall_effect)
                
                result = ExperimentResult(
                    experiment_id="exp03_segmentation",
                    hypothesis_id=hypothesis['id'],
                    control_group_size=int(control_total),
                    treatment_group_size=int(treatment_total),
                    control_metric=0.0,  # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æžã§ã¯å€‹åˆ¥å€¤ã¯æ„å‘³ãªã—
                    treatment_metric=0.0,
                    effect_size=overall_effect,
                    statistical_significance=significance,
                    confidence_interval=(0, 0)
                )
                
                print(f"âœ… exp03 å®Œäº†: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŠ¹æžœ {overall_effect:.4f}")
                return result
            else:
                print("âŒ exp03: ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                return None
                
        except Exception as e:
            print(f"âŒ exp03 å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    # ==========================================
    # Phase 3: çµ±åˆå®Ÿè¡Œ
    # ==========================================
    
    def run_complete_pipeline(self) -> Dict:
        """ðŸš€ å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ðŸš€ ç©¶æ¥µã®ä»®èª¬ç”Ÿæˆãƒ»æ”¹è‰¯ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        print(f"é–‹å§‹æ™‚åˆ»: {self.session_start}")
        print("=" * 60)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(SCHEMA_TXT_FILE, 'r', encoding='utf-8') as f:
            schema_text = f.read()
        with open(DATA_EXPLORATION_FILE, 'r', encoding='utf-8') as f:
            data_exploration = f.read()
        
        pipeline_results = {
            'session_info': {
                'start_time': self.session_start.isoformat(),
                'system_version': 'v1.0_ultimate'
            },
            'phase1_iterative_refinement': {},
            'phase2_systematic_experiments': {},
            'overall_summary': {}
        }
        
        # Phase 1: æ®µéšŽçš„æ”¹è‰¯ãƒ—ãƒ­ã‚»ã‚¹
        print("\nðŸŽ¯ PHASE 1: æ®µéšŽçš„ä»®èª¬æ”¹è‰¯ãƒ—ãƒ­ã‚»ã‚¹")
        print("=" * 60)
        
        # 1.1 åˆæœŸç”Ÿæˆ
        initial_hypotheses = self.generate_initial_hypotheses(schema_text, data_exploration)
        
        # 1.2-1.4 åå¾©æ”¹è‰¯
        current_hypotheses = initial_hypotheses
        evolution_history = []
        
        for round_num in range(self.refinement_rounds):
            print(f"\nðŸ”„ æ”¹è‰¯ãƒ©ã‚¦ãƒ³ãƒ‰ {round_num + 1}/{self.refinement_rounds}")
            
            # æ‰¹åˆ¤çš„è©•ä¾¡
            evaluations = self.critical_evaluation(current_hypotheses)
            
            # æ”¹è‰¯
            refined = self.refine_hypotheses(current_hypotheses, evaluations)
            
            evolution_history.append({
                'round': round_num + 1,
                'evaluations': evaluations,
                'refinements': refined
            })
            
            current_hypotheses = refined
        
        # æœ€çµ‚æ¤œè¨¼
        final_hypotheses = self.final_validation(current_hypotheses)
        
        pipeline_results['phase1_iterative_refinement'] = {
            'initial_hypotheses': initial_hypotheses,
            'evolution_history': evolution_history,
            'final_hypotheses': final_hypotheses
        }
        
        # Phase 2: ç³»çµ±çš„å®Ÿé¨“å®Ÿè¡Œ
        print("\nðŸ§ª PHASE 2: ç³»çµ±çš„å®Ÿé¨“å®Ÿè¡Œ")
        print("=" * 60)
        
        experiment_results = {}
        
        for hypothesis in final_hypotheses:
            if hypothesis.get('validation_status') == 'pass':
                print(f"\nðŸŽ¯ {hypothesis['id']} ã®ç³»çµ±çš„å®Ÿé¨“é–‹å§‹")
                
                # å®Ÿé¨“è¨­è¨ˆ
                design = self.design_experimental_groups(hypothesis)
                
                if design:
                    # 3æ®µéšŽå®Ÿé¨“å®Ÿè¡Œ
                    exp_results = {}
                    
                    # exp01
                    exp01 = self.execute_exp01_basic_analysis(hypothesis, design)
                    if exp01:
                        exp_results['exp01'] = asdict(exp01)
                    
                    # exp02  
                    exp02 = self.execute_exp02_comparative_analysis(hypothesis, design)
                    if exp02:
                        exp_results['exp02'] = asdict(exp02)
                    
                    # exp03
                    exp03 = self.execute_exp03_advanced_segmentation(hypothesis, design)
                    if exp03:
                        exp_results['exp03'] = asdict(exp03)
                    
                    experiment_results[hypothesis['id']] = {
                        'hypothesis': hypothesis,
                        'experimental_design': design,
                        'experiment_results': exp_results,
                        'summary': self._generate_experiment_summary(exp_results)
                    }
                else:
                    print(f"âŒ {hypothesis['id']}: å®Ÿé¨“è¨­è¨ˆå¤±æ•—")
            else:
                print(f"â­ï¸ {hypothesis['id']}: æ¤œè¨¼ãƒ‘ã‚¹ä¸åˆæ ¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        
        pipeline_results['phase2_systematic_experiments'] = experiment_results
        
        # å…¨ä½“ã‚µãƒžãƒªãƒ¼ç”Ÿæˆ
        pipeline_results['overall_summary'] = self._generate_overall_summary(
            pipeline_results['phase1_iterative_refinement'],
            pipeline_results['phase2_systematic_experiments']
        )
        
        # çµæžœä¿å­˜
        output_file = self.config.output.get_output_path('ultimate_hypothesis_results.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(pipeline_results, f, ensure_ascii=False, indent=2, default=str)
        
        print("\nðŸŽ‰ ç©¶æ¥µã‚·ã‚¹ãƒ†ãƒ å®Œäº†!")
        print("=" * 60)
        print(f"ðŸ“Š ç·å®Ÿè¡Œæ™‚é–“: {datetime.now() - self.session_start}")
        print(f"ðŸ“ çµæžœä¿å­˜: {output_file}")
        print(f"ðŸ“ˆ æœ€çµ‚ä»®èª¬æ•°: {len(final_hypotheses)}")
        print(f"ðŸ§ª å®Ÿé¨“å®Ÿè¡Œæ•°: {len(experiment_results)}")
        
        return pipeline_results
    
    def _generate_experiment_summary(self, exp_results: Dict) -> str:
        """å®Ÿé¨“çµæžœã‚µãƒžãƒªãƒ¼"""
        if not exp_results:
            return "å®Ÿé¨“å®Ÿè¡Œå¤±æ•—"
        
        summaries = []
        for exp_name, result in exp_results.items():
            effect = result.get('effect_size', 0)
            significance = result.get('statistical_significance', False)
            summaries.append(f"{exp_name}: åŠ¹æžœ{effect:.3f} {'(æœ‰æ„)' if significance else '(éžæœ‰æ„)'}")
        
        return " | ".join(summaries)
    
    def _generate_overall_summary(self, phase1_results: Dict, phase2_results: Dict) -> Dict:
        """å…¨ä½“ã‚µãƒžãƒªãƒ¼ç”Ÿæˆ"""
        return {
            'initial_hypotheses_count': len(phase1_results.get('initial_hypotheses', [])),
            'final_hypotheses_count': len(phase1_results.get('final_hypotheses', [])),
            'experiments_executed': len(phase2_results),
            'significant_findings': sum(
                1 for exp_data in phase2_results.values()
                for exp_result in exp_data.get('experiment_results', {}).values()
                if exp_result.get('statistical_significance', False)
            ),
            'avg_effect_sizes': [
                exp_result.get('effect_size', 0)
                for exp_data in phase2_results.values()
                for exp_result in exp_data.get('experiment_results', {}).values()
                if 'effect_size' in exp_result
            ]
        }
    
    def _extract_json_list(self, response: str) -> List[Dict]:
        """JSONãƒªã‚¹ãƒˆæŠ½å‡º"""
        import re
        json_match = re.search(r'```json\s*(\[.*?\])\s*```', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        json_match = re.search(r'(\[.*?\])', response, re.DOTALL)
        return json.loads(json_match.group(1)) if json_match else []
    
    def _extract_json_object(self, response: str) -> Dict:
        """JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŠ½å‡º"""
        import re
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        json_match = re.search(r'(\{.*?\})', response, re.DOTALL)
        return json.loads(json_match.group(1)) if json_match else {}

# å®Ÿè¡Œ
if __name__ == "__main__":
    system = UltimateHypothesisSystem()
    results = system.run_complete_pipeline()