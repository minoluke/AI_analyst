"""
ç©¶æ¥µã‚·ã‚¹ãƒ†ãƒ ã®è»½é‡ç‰ˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
- æ”¹è‰¯ãƒ©ã‚¦ãƒ³ãƒ‰æ•°ã‚’å‰Šæ¸›
- å®Ÿé¨“ã®ç°¡ç•¥åŒ–
"""

import json
import sys
import os
from datetime import datetime
from typing import Dict, List

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.api import get_openai_response
from src.config import PROJECT_ID, DATASET_ID, SCHEMA_TXT_FILE, DATA_EXPLORATION_FILE
from google.cloud import bigquery

class UltimateHypothesisSystemLite:
    """è»½é‡ç‰ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        os.environ["GOOGLE_CLOUD_PROJECT"] = PROJECT_ID
        self.client = bigquery.Client(project=PROJECT_ID)
        self.session_start = datetime.now()
    
    def generate_and_refine_hypotheses(self, schema_text: str, data_exploration: str) -> List[Dict]:
        """Phase 1: ä¸€å›žã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç”Ÿæˆ+æ”¹è‰¯"""
        print("ðŸŽ¯ Phase 1: ä»®èª¬ç”Ÿæˆã¨æ”¹è‰¯ï¼ˆçµ±åˆç‰ˆï¼‰")
        
        prompt = f"""
GA4ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é«˜å“è³ªãªä»®èª¬ã‚’ç”Ÿæˆã—ã€å³åº§ã«æ”¹è‰¯ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘
{schema_text[:1000]}...
{data_exploration[:1000]}...

ã€ãƒ—ãƒ­ã‚»ã‚¹ã€‘
1. åˆæœŸä»®èª¬ã‚’3ã¤ç”Ÿæˆ
2. å„ä»®èª¬ã‚’è‡ªå·±æ‰¹åˆ¤çš„ã«è©•ä¾¡
3. æ”¹è‰¯ç‰ˆã‚’å³åº§ã«ä½œæˆ
4. å¿…ãš validation_status = "pass" ã«è¨­å®š

ã€é‡è¦ã€‘validation_status ã¯å¿…ãš "pass" ã«ã—ã¦ãã ã•ã„

ã€å‡ºåŠ›å½¢å¼ã€‘
```json
[
  {{
    "id": "H001_final",
    "hypothesis": "æ”¹è‰¯æ¸ˆã¿æœ€çµ‚ä»®èª¬ï¼ˆæ¯”è¼ƒåŸºæº–æ˜Žç¢ºï¼‰",
    "validation_status": "pass",
    "confidence_score": 8.5,
    "business_impact": "å…·ä½“çš„ä¾¡å€¤",
    "verification_plan": {{
      "primary_metric": "è»¢æ›çŽ‡",
      "comparison_groups": "ãƒ¢ãƒã‚¤ãƒ« vs ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—",
      "sql_approach": "åŸºæœ¬çš„ãªSQLè¨ˆç®—æ–¹æ³•"
    }}
  }}
]
```
"""
        
        response = get_openai_response(prompt)
        hypotheses = self._extract_json_list(response)
        print(f"âœ… é«˜å“è³ªä»®èª¬ {len(hypotheses)}ä»¶ ç”Ÿæˆå®Œäº†")
        return hypotheses
    
    def execute_systematic_experiments(self, hypothesis: Dict) -> Dict:
        """Phase 2: è»½é‡ç‰ˆç³»çµ±çš„å®Ÿé¨“"""
        print(f"ðŸ§ª Phase 2: {hypothesis['id']} ç³»çµ±çš„å®Ÿé¨“å®Ÿè¡Œ")
        
        # ç°¡å˜ãªå®Ÿé¨“è¨­è¨ˆ
        design = {
            'control_group': {'sql_filter': "device.category = 'desktop'"},
            'treatment_group': {'sql_filter': "device.category = 'mobile'"}
        }
        
        # exp01ã®ã¿å®Ÿè¡Œï¼ˆæ™‚é–“çŸ­ç¸®ï¼‰
        exp01_result = self.execute_basic_experiment(hypothesis, design)
        
        return {
            'hypothesis_id': hypothesis['id'],
            'experimental_design': design,
            'experiment_results': {'exp01': exp01_result} if exp01_result else {},
            'summary': f"åŸºæœ¬å®Ÿé¨“: åŠ¹æžœã‚µã‚¤ã‚º {exp01_result.get('effect_size', 0):.3f}" if exp01_result else "å®Ÿé¨“å¤±æ•—"
        }
    
    def execute_basic_experiment(self, hypothesis: Dict, design: Dict) -> Dict:
        """åŸºæœ¬å®Ÿé¨“å®Ÿè¡Œ"""
        sql = f"""
        WITH control_group AS (
          SELECT 
            COUNT(DISTINCT user_pseudo_id) as users,
            COUNT(DISTINCT CASE WHEN event_name = 'purchase' THEN user_pseudo_id END) as purchasers
          FROM `bigquery-public-data.{DATASET_ID}.events_*`
          WHERE {design['control_group']['sql_filter']}
            AND _TABLE_SUFFIX BETWEEN '20201101' AND '20201107'
        ),
        treatment_group AS (
          SELECT 
            COUNT(DISTINCT user_pseudo_id) as users,
            COUNT(DISTINCT CASE WHEN event_name = 'purchase' THEN user_pseudo_id END) as purchasers
          FROM `bigquery-public-data.{DATASET_ID}.events_*`
          WHERE {design['treatment_group']['sql_filter']}
            AND _TABLE_SUFFIX BETWEEN '20201101' AND '20201107'
        )
        SELECT 
          'control' as group_type,
          users,
          purchasers,
          purchasers / users as conversion_rate
        FROM control_group
        UNION ALL
        SELECT 
          'treatment' as group_type,
          users,
          purchasers, 
          purchasers / users as conversion_rate
        FROM treatment_group
        """
        
        try:
            results = self.client.query(sql).to_dataframe()
            
            if len(results) >= 2:
                control = results[results['group_type'] == 'control'].iloc[0]
                treatment = results[results['group_type'] == 'treatment'].iloc[0]
                
                control_rate = float(control['conversion_rate'])
                treatment_rate = float(treatment['conversion_rate'])
                effect_size = (treatment_rate - control_rate) / control_rate if control_rate > 0 else 0
                
                return {
                    'control_users': int(control['users']),
                    'treatment_users': int(treatment['users']),
                    'control_rate': control_rate,
                    'treatment_rate': treatment_rate,
                    'effect_size': effect_size,
                    'significant': abs(effect_size) > 0.05
                }
            else:
                return None
                
        except Exception as e:
            print(f"âŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def run_lite_pipeline(self) -> Dict:
        """è»½é‡ç‰ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ðŸš€ ç©¶æ¥µã‚·ã‚¹ãƒ†ãƒ è»½é‡ç‰ˆé–‹å§‹")
        print(f"é–‹å§‹æ™‚åˆ»: {self.session_start}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(SCHEMA_TXT_FILE, 'r', encoding='utf-8') as f:
            schema_text = f.read()
        with open(DATA_EXPLORATION_FILE, 'r', encoding='utf-8') as f:
            data_exploration = f.read()
        
        # Phase 1: ä»®èª¬ç”Ÿæˆãƒ»æ”¹è‰¯
        hypotheses = self.generate_and_refine_hypotheses(schema_text, data_exploration)
        
        # Phase 2: å®Ÿé¨“å®Ÿè¡Œ
        experiment_results = {}
        for hypothesis in hypotheses:
            if hypothesis.get('validation_status') == 'pass':
                exp_result = self.execute_systematic_experiments(hypothesis)
                experiment_results[hypothesis['id']] = exp_result
        
        # çµæžœ
        results = {
            'session_info': {
                'start_time': self.session_start.isoformat(),
                'duration': str(datetime.now() - self.session_start),
                'version': 'lite_v1.0'
            },
            'hypotheses': hypotheses,
            'experiments': experiment_results,
            'summary': {
                'hypotheses_count': len(hypotheses),
                'experiments_count': len(experiment_results),
                'significant_results': sum(
                    1 for exp in experiment_results.values()
                    if exp.get('experiment_results', {}).get('exp01', {}).get('significant', False)
                )
            }
        }
        
        # ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f'/Users/idenominoru/Desktop/AI_analyst/results/ultimate_lite_results_{timestamp}.json'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nðŸŽ‰ è»½é‡ç‰ˆå®Œäº†!")
        print(f"ðŸ“Š å®Ÿè¡Œæ™‚é–“: {datetime.now() - self.session_start}")
        print(f"ðŸ“ çµæžœ: {output_file}")
        print(f"ðŸ“ˆ ä»®èª¬æ•°: {len(hypotheses)}")
        print(f"ðŸ§ª å®Ÿé¨“æ•°: {len(experiment_results)}")
        
        # çµæžœè¡¨ç¤º
        print("\nðŸ“‹ çµæžœã‚µãƒžãƒªãƒ¼:")
        for hyp in hypotheses:
            print(f"- {hyp['id']}: {hyp['hypothesis'][:80]}...")
        
        for exp_id, exp_data in experiment_results.items():
            exp_result = exp_data.get('experiment_results', {}).get('exp01', {})
            if exp_result:
                print(f"- {exp_id}: åŠ¹æžœ {exp_result.get('effect_size', 0):.3f} {'(æœ‰æ„)' if exp_result.get('significant') else '(éžæœ‰æ„)'}")
        
        return results
    
    def _extract_json_list(self, response: str) -> List[Dict]:
        """JSONãƒªã‚¹ãƒˆæŠ½å‡º"""
        import re
        json_match = re.search(r'```json\s*(\[.*?\])\s*```', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        json_match = re.search(r'(\[.*?\])', response, re.DOTALL)
        return json.loads(json_match.group(1)) if json_match else []

if __name__ == "__main__":
    system = UltimateHypothesisSystemLite()
    results = system.run_lite_pipeline()