import os
import json
import pandas as pd
import sys
import logging
import asyncio
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import concurrent.futures

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.api import get_openai_response_async
from google.cloud import bigquery
from src.config import (
    PROJECT_ID,
    LLM_RESPONSES_DIR,
    SCHEMA_TXT_FILE,
    DATA_EXPLORATION_FILE,
    QUANTITATIVE_REPORT_FILE,
    HYPOTHESES_FILE,
    HYPOTHESIS_REPORTS_DIR
)
from src.api_monitor import api_monitor

# â€•â€•â€• ãƒ­ã‚°è¨­å®š â€•â€•â€•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('parallel_hypothesis_validation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# â€•â€•â€• è¨­å®š â€•â€•â€•
SQL_RETRY_LIMIT = 10
ANALYSIS_RETRY_LIMIT = 5
MIN_REQUIRED_ROWS = 1
MAX_CONCURRENT_REQUESTS = 3  # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™

os.environ["GOOGLE_CLOUD_PROJECT"] = PROJECT_ID
client = bigquery.Client(project=PROJECT_ID)

class ParallelHypothesisValidationPipeline:
    """ä¸¦åˆ—å‡¦ç†å¯¾å¿œã®ä»®èª¬æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self):
        self.schema_text = self.load_file(SCHEMA_TXT_FILE)
        self.data_exploration = self.load_file(DATA_EXPLORATION_FILE)
        self.hypotheses = self.load_hypotheses()
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        
    def load_file(self, filepath: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                return f.read().strip()
        except FileNotFoundError:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
            return ""
    
    def load_hypotheses(self) -> List[Dict]:
        """ä»®èª¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(HYPOTHESES_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"ä»®èª¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {HYPOTHESES_FILE}")
            return []
    
    async def generate_analysis_plan_async(self, hypothesis: Dict) -> List[Dict]:
        """ä»®èª¬ã«åŸºã¥ã„ã¦åˆ†æè¨ˆç”»ã‚’éåŒæœŸç”Ÿæˆ"""
        async with self.semaphore:  # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™
            
            prompt = f"""
ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ä»®èª¬ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®åˆ†æè¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚

## ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ å®šç¾©
```
{self.schema_text}
```

## å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹
```
{self.data_exploration}
```

## æ¤œè¨¼ã™ã‚‹ä»®èª¬
```json
{json.dumps(hypothesis, ensure_ascii=False, indent=2)}
```

## è¦ä»¶
ã“ã®ä»®èª¬ã‚’é©åˆ‡ã«æ¤œè¨¼ã™ã‚‹ãŸã‚ã«ã€æœ€å¤§3ã¤ã®SQLã‚¯ã‚¨ãƒªã‹ã‚‰ãªã‚‹åˆ†æè¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚
å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯æ¯”è¼ƒå¯¾è±¡ã‚’æ˜ç¢ºã«ã—ã€ä»®èª¬ã®å¦¥å½“æ€§ã‚’å¤šè§’çš„ã«æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚

é‡è¦: å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆåã®ã¿ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š
- page_view, user_engagement, scroll, view_item, session_start
- add_to_cart, begin_checkout, add_shipping_info, add_payment_info, purchase

ä»¥ä¸‹ã®JSONå½¢å¼ã§åˆ†æè¨ˆç”»ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

```json
{{
  "analysis_steps": [
    {{
      "step_id": "step1",
      "title": "åŸºæœ¬åˆ†æ",
      "purpose": "åŸºæœ¬çš„ãªé·ç§»ç‡ã‚’æ¸¬å®š",
      "sql_requirements": [
        "å®Ÿéš›ã®ã‚¤ãƒ™ãƒ³ãƒˆåã‚’ä½¿ç”¨",
        "é·ç§»ç‡ã‚’è¨ˆç®—"
      ]
    }},
    {{
      "step_id": "step2", 
      "title": "æ¯”è¼ƒåˆ†æ",
      "purpose": "å¯¾ç…§ç¾¤ã¨ã®æ¯”è¼ƒ",
      "sql_requirements": [
        "æ¡ä»¶åˆ¥ã®æ¯”è¼ƒ",
        "çµ±è¨ˆçš„ãªå·®ç•°ã‚’ç¢ºèª"
      ]
    }}
  ]
}}
```

JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
"""
            
            hyp_id = hypothesis.get("id", "UNKNOWN")
            response = await get_openai_response_async(
                prompt, 
                request_type="analysis_plan",
                hypothesis_id=hyp_id
            )
            
            # JSONã®æŠ½å‡º
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            
            try:
                return json.loads(response.strip())["analysis_steps"]
            except (json.JSONDecodeError, KeyError) as e:
                logger.error(f"åˆ†æè¨ˆç”»ã®è§£æã«å¤±æ•—: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return [
                    {
                        "step_id": "step1",
                        "title": "åŸºæœ¬åˆ†æ", 
                        "purpose": "åŸºæœ¬çš„ãªé·ç§»ç‡åˆ†æ",
                        "sql_requirements": ["åŸºæœ¬é·ç§»ç‡ã‚’è¨ˆç®—"]
                    }
                ]
    
    async def generate_sql_for_step_async(self, hypothesis: Dict, analysis_step: Dict, error_message: str = "") -> str:
        """åˆ†æã‚¹ãƒ†ãƒƒãƒ—ã«åŸºã¥ã„ã¦SQLã‚’éåŒæœŸç”Ÿæˆ"""
        async with self.semaphore:  # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™
            
            retry_context = ""
            if error_message:
                retry_context = f"""
**å‰å›ã®SQLå®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:**
```
{error_message}
```
ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ã€æ­£ã—ãå‹•ä½œã™ã‚‹SQLã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""
            
            prompt = f"""
ã‚ãªãŸã¯BigQueryã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€åˆ†æã‚¹ãƒ†ãƒƒãƒ—ã«å¯¾å¿œã™ã‚‹SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

## ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ å®šç¾©
```
{self.schema_text}
```

## å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹
```
{self.data_exploration}
```

## æ¤œè¨¼ã™ã‚‹ä»®èª¬
```json
{json.dumps(hypothesis, ensure_ascii=False, indent=2)}
```

## å®Ÿè¡Œã™ã‚‹åˆ†æã‚¹ãƒ†ãƒƒãƒ—
- **ã‚¹ãƒ†ãƒƒãƒ—ID**: {analysis_step['step_id']}
- **ã‚¿ã‚¤ãƒˆãƒ«**: {analysis_step['title']}
- **ç›®çš„**: {analysis_step['purpose']}
- **è¦ä»¶**: {', '.join(analysis_step['sql_requirements'])}

{retry_context}

## SQLç”Ÿæˆè¦ä»¶
1. å¿…ãšå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆåã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
2. WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131' ã‚’å«ã‚ã¦ãã ã•ã„
3. åˆ†æã‚¹ãƒ†ãƒƒãƒ—ã®ç›®çš„ã«æ²¿ã£ãŸé©åˆ‡ãªã‚«ãƒ©ãƒ ã‚’å«ã‚ã¦ãã ã•ã„
4. å¿…è¦ã«å¿œã˜ã¦ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚„é›†è¨ˆã‚’è¡Œã£ã¦ãã ã•ã„
5. çµæœãŒæ¯”è¼ƒã—ã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„

SQLã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ã¯ä¸è¦ï¼‰ï¼š
"""
            
            hyp_id = hypothesis.get("id", "UNKNOWN")
            step_id = analysis_step["step_id"]
            
            return await get_openai_response_async(
                prompt,
                request_type="sql_generation",
                hypothesis_id=hyp_id,
                step_id=step_id
            )
    
    def execute_sql_sync(self, sql: str) -> Tuple[Optional[pd.DataFrame], str]:
        """SQLã‚’åŒæœŸå®Ÿè¡Œ"""
        try:
            job = client.query(sql)
            result_df = job.to_dataframe()
            
            if result_df.empty:
                return None, "ã‚¯ã‚¨ãƒªçµæœãŒç©ºã§ã™"
            
            if len(result_df) < MIN_REQUIRED_ROWS:
                return None, f"çµæœè¡Œæ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {len(result_df)}è¡Œ"
            
            return result_df, ""
            
        except Exception as e:
            return None, f"SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    async def execute_sql_with_retry_async(self, hypothesis: Dict, analysis_step: Dict) -> Tuple[Optional[pd.DataFrame], str]:
        """SQLã‚’éåŒæœŸå®Ÿè¡Œï¼ˆå†å¸°çš„ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
        hyp_id = hypothesis.get("id", "UNKNOWN")
        step_id = analysis_step["step_id"]
        logger.info(f"ğŸš€ {hyp_id}-{step_id}: SQLç”Ÿæˆã¨å®Ÿè¡Œã‚’é–‹å§‹")
        
        error_message = ""
        
        for attempt in range(1, SQL_RETRY_LIMIT + 1):
            logger.info(f"  è©¦è¡Œ {attempt}/{SQL_RETRY_LIMIT}")
            
            # SQLç”Ÿæˆï¼ˆéåŒæœŸï¼‰
            try:
                sql = await self.generate_sql_for_step_async(hypothesis, analysis_step, error_message)
                
                # SQLã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
                if sql.startswith("```sql"):
                    sql = sql[6:]
                if sql.endswith("```"):
                    sql = sql[:-3]
                sql = sql.strip()
                
                if not sql or len(sql.strip()) < 20:
                    raise ValueError("ç”Ÿæˆã•ã‚ŒãŸSQLãŒçŸ­ã™ãã¾ã™")
                
                # SQLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                sql_file = os.path.join(LLM_RESPONSES_DIR, f"{hyp_id}_{step_id}.json")
                with open(sql_file, "w", encoding="utf-8") as f:
                    json.dump({
                        "sql": sql,
                        "step": analysis_step,
                        "attempt": attempt,
                        "timestamp": datetime.now().isoformat()
                    }, f, ensure_ascii=False, indent=2)
                
                logger.info(f"  SQLç”Ÿæˆå®Œäº†: {len(sql)}æ–‡å­—")
                
            except Exception as e:
                error_message = f"SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
                logger.warning(f"  {error_message}")
                continue
            
            # SQLå®Ÿè¡Œï¼ˆéåŒæœŸï¼‰
            loop = asyncio.get_event_loop()
            result_df, sql_error = await loop.run_in_executor(
                None, self.execute_sql_sync, sql
            )
            
            if result_df is not None:
                logger.info(f"  âœ… SQLå®Ÿè¡ŒæˆåŠŸ: {len(result_df)}è¡Œã®çµæœ")
                return result_df, ""
            else:
                error_message = sql_error
                logger.warning(f"  âš ï¸ {error_message}")
                
                if attempt == SQL_RETRY_LIMIT:
                    logger.error(f"  âŒ {hyp_id}-{step_id}: {SQL_RETRY_LIMIT}å›è©¦è¡Œå¾Œã‚‚å¤±æ•—")
                    return None, error_message
        
        return None, error_message
    
    async def generate_comprehensive_analysis_report_async(self, hypothesis: Dict, analysis_results: List[Tuple[Dict, pd.DataFrame]]) -> str:
        """è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’çµ±åˆã—ãŸåŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’éåŒæœŸç”Ÿæˆ"""
        async with self.semaphore:  # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™
            
            # å…¨çµæœã‚’æ–‡å­—åˆ—åŒ–
            results_summary = ""
            for step, result_df in analysis_results:
                results_summary += f"\n\n### {step['title']} ({step['step_id']})\n"
                results_summary += f"ç›®çš„: {step['purpose']}\n\n"
                results_summary += "ãƒ‡ãƒ¼ã‚¿:\n"
                results_summary += result_df.to_string(index=False)
                results_summary += "\n\nçµ±è¨ˆã‚µãƒãƒªãƒ¼:\n"
                results_summary += result_df.describe().to_string()
            
            prompt = f"""
ä»¥ä¸‹ã®è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†æçµæœã‚’çµ±åˆã—ã¦ã€ä»®èª¬æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## æ¤œè¨¼ã—ãŸä»®èª¬
```json
{json.dumps(hypothesis, ensure_ascii=False, indent=2)}
```

## è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†æçµæœ
{results_summary}

## ãƒ¬ãƒãƒ¼ãƒˆè¦ä»¶
1. ä»®èª¬ãŒæ¤œè¨¼ã•ã‚ŒãŸã‹ã©ã†ã‹ã‚’ç«¯çš„ã«è¿°ã¹ã¦ãã ã•ã„
2. åˆ†æã®éç¨‹ã¨å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
3. æ¯”è¼ƒåˆ†æãŒã‚ã‚Œã°ã€ãã®å·®ç•°ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„
4. ä»®èª¬ã®æ¤œè¨¼çµæœã‚’æ•°å€¤çš„æ ¹æ‹ ã¨ã¨ã‚‚ã«è¿°ã¹ã¦ãã ã•ã„
5. å°‚é–€çš„ã™ããšã€ã‚ã‹ã‚Šã‚„ã™ãæ›¸ã„ã¦ãã ã•ã„

## é‡è¦ãªæ•°å€¤è¡¨ç¾ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
- æ•°å€¤ã‚’ã€Œé«˜ã„ã€ã€Œä½ã„ã€ã¨è©•ä¾¡ã™ã‚‹éš›ã¯ã€å¿…ãšæ¯”è¼ƒã®åŸºæº–ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„
- ä¾‹: âœ…ã€Œãƒã‚¦ãƒ³ã‚¹ç‡10.39%ã¯æ¥­ç•Œå¹³å‡15%ã¨æ¯”è¼ƒã—ã¦ä½ã„ã€
- ä¾‹: âŒã€Œãƒã‚¦ãƒ³ã‚¹ç‡ã¯é«˜ã„10.39%ã€
- è¤‡æ•°ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã§æ¯”è¼ƒãŒã‚ã‚‹å ´åˆ: ã€ŒAã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®10.39%ã«å¯¾ã—ã¦Bã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯15.2%ã§4.81ãƒã‚¤ãƒ³ãƒˆé«˜ã„ã€
- å˜ä¸€ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã—ã‹ãªã„å ´åˆ: å®¢è¦³çš„ã«æ•°å€¤ã‚’å ±å‘Šã—ã€çµ¶å¯¾çš„ãªè©•ä¾¡ï¼ˆé«˜ã„/ä½ã„ï¼‰ã¯é¿ã‘ã¦ãã ã•ã„
- ã€Œã‚ˆã‚Šé«˜ã„ã€ã€Œã‚ˆã‚Šä½ã„ã€ãªã©ç›¸å¯¾çš„ãªè¡¨ç¾ã‚’å„ªå…ˆã—ã¦ãã ã•ã„

æ³¨æ„: ãƒ“ã‚¸ãƒã‚¹ä¸Šã®ç¤ºå”†ã‚„æ”¹å–„ææ¡ˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

æ—¥æœ¬èªã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
"""
            
            hyp_id = hypothesis.get("id", "UNKNOWN")
            return await get_openai_response_async(
                prompt,
                request_type="comprehensive_report",
                hypothesis_id=hyp_id
            )
    
    async def process_single_hypothesis_async(self, hypothesis: Dict) -> bool:
        """å˜ä¸€ä»®èª¬ã®éåŒæœŸå‡¦ç†"""
        hyp_id = hypothesis.get("id", "UNKNOWN")
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¯ å‡¦ç†é–‹å§‹: {hyp_id}")
        logger.info(f"{'='*60}")
        
        try:
            # åˆ†æè¨ˆç”»ã®ç”Ÿæˆ
            logger.info(f"ğŸ“‹ {hyp_id}: åˆ†æè¨ˆç”»ã‚’ç”Ÿæˆä¸­...")
            analysis_plan = await self.generate_analysis_plan_async(hypothesis)
            logger.info(f"  ğŸ“‹ åˆ†æè¨ˆç”»: {len(analysis_plan)}ã‚¹ãƒ†ãƒƒãƒ—")
            
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆãŸã ã—åŒæ™‚æ•°åˆ¶é™ã‚ã‚Šï¼‰
            analysis_results = []
            
            # ã‚¹ãƒ†ãƒƒãƒ—ã‚’é †æ¬¡å®Ÿè¡Œï¼ˆSQLã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ï¼‰
            for step in analysis_plan:
                step_id = step["step_id"]
                logger.info(f"\n  ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ: {step_id} - {step['title']}")
                
                result_df, error = await self.execute_sql_with_retry_async(hypothesis, step)
                if result_df is None:
                    logger.error(f"    âŒ {step_id}: SQLå®Ÿè¡Œã«å¤±æ•— - {error}")
                    continue
                
                analysis_results.append((step, result_df))
                logger.info(f"    âœ… {step_id}: å®Œäº†")
            
            if not analysis_results:
                logger.error(f"âŒ {hyp_id}: å…¨ã‚¹ãƒ†ãƒƒãƒ—ãŒå¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            logger.info(f"ğŸ“Š {hyp_id}: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
            report = await self.generate_comprehensive_analysis_report_async(hypothesis, analysis_results)
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            report_file = os.path.join(HYPOTHESIS_REPORTS_DIR, f"{hyp_id}_parallel_comprehensive_report.txt")
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            
            logger.info(f"âœ… {hyp_id}: å‡¦ç†å®Œäº† - {report_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {hyp_id}: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {str(e)}")
            return False
    
    async def run_pipeline_async(self) -> None:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³éåŒæœŸå®Ÿè¡Œ"""
        start_time = datetime.now()
        logger.info(f"\nğŸš€ ä¸¦åˆ—ä»®èª¬æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æ¤œè¨¼å¯¾è±¡: {len(self.hypotheses)}ä»¶ã®ä»®èª¬")
        logger.info(f"æœ€å¤§åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {MAX_CONCURRENT_REQUESTS}")
        
        # å…¨ä»®èª¬ã‚’ä¸¦åˆ—å‡¦ç†
        tasks = [
            self.process_single_hypothesis_async(hypothesis)
            for hypothesis in self.hypotheses
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æˆåŠŸã—ãŸä»®èª¬ã‚’é›†è¨ˆ
        successful_hypotheses = []
        for i, (hypothesis, result) in enumerate(zip(self.hypotheses, results)):
            if isinstance(result, bool) and result:
                successful_hypotheses.append(hypothesis.get("id", f"H{i+1:03d}"))
            elif isinstance(result, Exception):
                logger.error(f"ä»®èª¬ {hypothesis.get('id', f'H{i+1:03d}')} ã§ã‚¨ãƒ©ãƒ¼: {result}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ä¿å­˜
        session_summary = await api_monitor.save_session_summary("parallel_session_summary.json")
        
        # çµæœã‚µãƒãƒªãƒ¼
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ‰ ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
        logger.info(f"{'='*60}")
        logger.info(f"å®Ÿè¡Œæ™‚é–“: {duration}")
        logger.info(f"æˆåŠŸ: {len(successful_hypotheses)}/{len(self.hypotheses)} ä»¶")
        logger.info(f"æˆåŠŸç‡: {len(successful_hypotheses)/len(self.hypotheses)*100:.1f}%")
        logger.info(f"ç·APIåˆ©ç”¨æ–™é‡‘: ${session_summary['total_cost_usd']:.4f}")
        logger.info(f"å¹³å‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆå˜ä¾¡: ${session_summary['average_cost_per_request']:.4f}")
        
        if successful_hypotheses:
            logger.info(f"æˆåŠŸã—ãŸä»®èª¬: {', '.join(successful_hypotheses)}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        pipeline = ParallelHypothesisValidationPipeline()
        asyncio.run(pipeline.run_pipeline_async())
    except Exception as e:
        logger.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()