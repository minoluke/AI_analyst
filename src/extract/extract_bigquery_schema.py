from google.cloud import bigquery
import pandas as pd
import sys
import os
from datetime import datetime
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.config import PROJECT_ID, DATASET_ID, SCHEMA_CSV_FILE, SCHEMA_TXT_FILE, DATA_EXPLORATION_FILE

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†è§£
dataset_parts = DATASET_ID.split(".")
source_project_id = dataset_parts[0] if len(dataset_parts) > 1 else PROJECT_ID
dataset_name = dataset_parts[1] if len(dataset_parts) > 1 else dataset_parts[0]

# è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
client = bigquery.Client(project=PROJECT_ID)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‚ç…§
dataset_ref = bigquery.DatasetReference(source_project_id, dataset_name)

def explore_data_content():
    """å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹ã‚’æ¢ç´¢ã—ã¦LLMç”¨ã®æƒ…å ±ã‚’åé›†"""
    
    # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã‚¯ã‚¨ãƒª
    base_table = f"`{source_project_id}.{dataset_name}.events_*`"
    
    explorations = {}
    
    print("ğŸ” GA4ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã‚’é–‹å§‹...")
    
    # 1. åˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆåã¨ãã®é »åº¦
    try:
        query = f"""
        SELECT 
            event_name,
            COUNT(*) as event_count,
            COUNT(DISTINCT user_pseudo_id) as unique_users
        FROM {base_table}
        WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'
        GROUP BY event_name
        ORDER BY event_count DESC
        LIMIT 20
        """
        result = client.query(query).to_dataframe()
        explorations['available_events'] = result.to_dict('records')
        print(f"âœ… ã‚¤ãƒ™ãƒ³ãƒˆç¨®é¡: {len(result)}å€‹ç™ºè¦‹")
    except Exception as e:
        print(f"âš ï¸ ã‚¤ãƒ™ãƒ³ãƒˆæ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        explorations['available_events'] = []
    
    # 2. ãƒ‡ãƒã‚¤ã‚¹ã‚«ãƒ†ã‚´ãƒª
    try:
        query = f"""
        SELECT 
            device.category as device_category,
            COUNT(*) as event_count,
            COUNT(DISTINCT user_pseudo_id) as unique_users
        FROM {base_table}
        WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'
        AND device.category IS NOT NULL
        GROUP BY device.category
        ORDER BY event_count DESC
        """
        result = client.query(query).to_dataframe()
        explorations['device_categories'] = result.to_dict('records')
        print(f"âœ… ãƒ‡ãƒã‚¤ã‚¹ã‚«ãƒ†ã‚´ãƒª: {len(result)}å€‹ç™ºè¦‹")
    except Exception as e:
        print(f"âš ï¸ ãƒ‡ãƒã‚¤ã‚¹æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        explorations['device_categories'] = []
    
    # 3. æµå…¥å…ƒæƒ…å ±
    try:
        query = f"""
        SELECT 
            traffic_source.source,
            traffic_source.medium,
            COUNT(*) as event_count
        FROM {base_table}
        WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'
        AND traffic_source.source IS NOT NULL
        GROUP BY traffic_source.source, traffic_source.medium
        ORDER BY event_count DESC
        LIMIT 15
        """
        result = client.query(query).to_dataframe()
        explorations['traffic_sources'] = result.to_dict('records')
        print(f"âœ… æµå…¥å…ƒ: {len(result)}å€‹ç™ºè¦‹")
    except Exception as e:
        print(f"âš ï¸ æµå…¥å…ƒæ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        explorations['traffic_sources'] = []
    
    # 4. ã‚ˆãä½¿ã‚ã‚Œã‚‹event_params
    try:
        query = f"""
        SELECT 
            param.key as param_key,
            COUNT(*) as usage_count
        FROM {base_table},
        UNNEST(event_params) as param
        WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'
        GROUP BY param.key
        ORDER BY usage_count DESC
        LIMIT 20
        """
        result = client.query(query).to_dataframe()
        explorations['common_event_params'] = result.to_dict('records')
        print(f"âœ… ã‚¤ãƒ™ãƒ³ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {len(result)}å€‹ç™ºè¦‹")
    except Exception as e:
        print(f"âš ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        explorations['common_event_params'] = []
    
    # 5. ãƒ‡ãƒ¼ã‚¿æœŸé–“ã¨è¦æ¨¡
    try:
        query = f"""
        SELECT 
            MIN(event_date) as min_date,
            MAX(event_date) as max_date,
            COUNT(*) as total_events,
            COUNT(DISTINCT user_pseudo_id) as total_users
        FROM {base_table}
        WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'
        """
        result = client.query(query).to_dataframe()
        explorations['data_summary'] = result.to_dict('records')[0]
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿è¦æ¨¡æƒ…å ±ã‚’å–å¾—")
    except Exception as e:
        print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿è¦æ¨¡æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        explorations['data_summary'] = {}
    
    # 6. åœ°ç†æƒ…å ±
    try:
        query = f"""
        SELECT 
            geo.country,
            COUNT(*) as event_count
        FROM {base_table}
        WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'
        AND geo.country IS NOT NULL
        GROUP BY geo.country
        ORDER BY event_count DESC
        LIMIT 10
        """
        result = client.query(query).to_dataframe()
        explorations['top_countries'] = result.to_dict('records')
        print(f"âœ… åœ°ç†æƒ…å ±: {len(result)}å€‹ç™ºè¦‹")
    except Exception as e:
        print(f"âš ï¸ åœ°ç†æƒ…å ±æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        explorations['top_countries'] = []
    
    return explorations

def generate_schema_file(schema_df):
    """ç´”ç²‹ãªãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©æ›¸ã¨ã—ã¦ã®ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
    
    # æ—¥æ¬¡åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’çµ±åˆï¼ˆã™ã¹ã¦åŒã˜æ§‹é€ ï¼‰
    # events_YYYYMMDD ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
    events_tables = [t for t in schema_df['Table'].unique() if t.startswith('events_')]
    other_tables = [t for t in schema_df['Table'].unique() if not t.startswith('events_')]
    
    schema_content = f"""# GA4 Table Schema Definition
Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Dataset: {DATASET_ID}

## ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ å®šç¾©

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ GA4 events ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ å®šç¾©ï¼ˆã‚¹ã‚­ãƒ¼ãƒï¼‰ã®ã¿ã‚’è¨˜è¼‰ã—ã¦ã„ã¾ã™ã€‚
å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹ã«ã¤ã„ã¦ã¯ data_exploration.txt ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

"""
    
    # eventsãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ—¥æ¬¡åˆ†å‰²ï¼‰ã‚’çµ±åˆã—ã¦è¡¨ç¤º
    if events_tables:
        # æœ€åˆã®eventsãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã‚’ä»£è¡¨ã¨ã—ã¦ä½¿ç”¨
        first_events_table = events_tables[0]
        table_fields = schema_df[schema_df['Table'] == first_events_table]
        
        schema_content += f"""
## events_* (æ—¥æ¬¡åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«)

**æ³¨æ„**: ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¯æ—¥ä»˜ã”ã¨ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ï¼ˆevents_20201101, events_20201102, ... events_20210131ï¼‰
ã™ã¹ã¦åŒã˜æ§‹é€ ã‚’æŒã¤ãŸã‚ã€ä»¥ä¸‹ã«ä»£è¡¨çš„ãªæ§‹é€ ã‚’ç¤ºã—ã¾ã™ã€‚

**ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {len(events_tables)}å€‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«
**æœŸé–“**: {min(events_tables).replace('events_', '')} ï½ {max(events_tables).replace('events_', '')}

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å | ãƒ‡ãƒ¼ã‚¿å‹ | ãƒ¢ãƒ¼ãƒ‰ | èª¬æ˜ |
|-------------|---------|-------|------|"""
        
        for _, field in table_fields.iterrows():
            description = get_field_description(field['Field'])
            schema_content += f"""
| {field['Field']} | {field['Type']} | {field['Mode']} | {description} |"""
        
        schema_content += f"""

### ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è©³ç´°æ§‹é€ 

ä»¥ä¸‹ã®RECORDã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯è¤‡é›‘ãªæ§‹é€ ã‚’æŒã¡ã¾ã™ï¼š

"""
        
        # RECORDã‚¿ã‚¤ãƒ—ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è©³ç´°ã‚’åˆ¥é€”è¨˜è¼‰
        record_fields = table_fields[table_fields['Type'] == 'RECORD']
        for _, record_field in record_fields.iterrows():
            nested_fields = get_nested_fields(schema_df, first_events_table, record_field['Field'])
            if nested_fields:
                schema_content += f"""
#### {record_field['Field']}
| ã‚µãƒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | ãƒ‡ãƒ¼ã‚¿å‹ | ãƒ¢ãƒ¼ãƒ‰ |
|--------------|---------|-------|"""
                for nested in nested_fields:
                    schema_content += f"""
| {nested['Field']} | {nested['Type']} | {nested['Mode']} |"""
                schema_content += f"""

"""
    
    # ãã®ä»–ã®ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
    for table_name in other_tables:
        table_fields = schema_df[schema_df['Table'] == table_name]
        schema_content += f"""
## {table_name}

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å | ãƒ‡ãƒ¼ã‚¿å‹ | ãƒ¢ãƒ¼ãƒ‰ | èª¬æ˜ |
|-------------|---------|-------|------|"""
        
        for _, field in table_fields.iterrows():
            description = get_field_description(field['Field'])
            schema_content += f"""
| {field['Field']} | {field['Type']} | {field['Mode']} | {description} |"""
        
        schema_content += f"""

"""
    
    schema_content += f"""
## ã‚¯ã‚¨ãƒªæ™‚ã®é‡è¦ãªæ³¨æ„äº‹é …

1. **æ—¥æ¬¡åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‚ç…§æ–¹æ³•**:
   - å˜ä¸€æ—¥: `{DATASET_ID}.events_20201101`
   - è¤‡æ•°æ—¥ï¼ˆæ¨å¥¨ï¼‰: `{DATASET_ID}.events_*`
   - æœŸé–“æŒ‡å®š: `WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'`

2. **ãƒ‡ãƒ¼ã‚¿å‹ã«ã¤ã„ã¦**:
   - RECORDã‚¿ã‚¤ãƒ—: ãƒã‚¹ãƒˆã•ã‚ŒãŸæ§‹é€ ï¼ˆä¸Šè¨˜è©³ç´°å‚ç…§ï¼‰
   - REPEATEDãƒ¢ãƒ¼ãƒ‰: é…åˆ—ã¨ã—ã¦æ ¼ç´
   - NULLABLEãƒ¢ãƒ¼ãƒ‰: NULLå€¤ã‚’è¨±å¯

3. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**:
   - å¿…ãšæ—¥ä»˜ç¯„å›²ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆ_TABLE_SUFFIXæ¡ä»¶ï¼‰
   - å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’ SELECT ã—ã¦ãã ã•ã„

## å‚è€ƒãƒªãƒ³ã‚¯

- å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹ã¨SQLä¾‹: data_exploration.txt
- BigQueryå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://cloud.google.com/bigquery/docs/
- GA4 BigQuery Export ã‚¹ã‚­ãƒ¼ãƒ: https://support.google.com/analytics/answer/7029846
"""
    
    return schema_content

def get_nested_fields(schema_df, table_name, parent_field):
    """ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ§‹é€ ã‚’å–å¾—"""
    # BigQueryã®ã‚¹ã‚­ãƒ¼ãƒã§ã¯ã€ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ parent.child ã®å½¢å¼ã§è¡¨ç¾ã•ã‚Œã‚‹
    nested_pattern = f"{parent_field}."
    nested_fields = schema_df[
        (schema_df['Table'] == table_name) & 
        (schema_df['Field'].str.startswith(nested_pattern))
    ]
    
    # ç›´æ¥ã®å­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’å–å¾—ï¼ˆå­«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯é™¤å¤–ï¼‰
    direct_children = []
    for _, field in nested_fields.iterrows():
        field_name = field['Field']
        # parent_fieldã‚’é™¤å»ã—ãŸéƒ¨åˆ†
        child_part = field_name[len(nested_pattern):]
        # ã•ã‚‰ã«ãƒã‚¹ãƒˆã—ã¦ã„ãªã„ï¼ˆ.ãŒå«ã¾ã‚Œã¦ã„ãªã„ï¼‰ã‚‚ã®ã ã‘
        if '.' not in child_part:
            direct_children.append({
                'Field': child_part,
                'Type': field['Type'],
                'Mode': field['Mode']
            })
    
    return direct_children

def generate_data_exploration_file(explorations):
    """ãƒ‡ãƒ¼ã‚¿æ¢ç´¢çµæœã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ç”Ÿæˆ"""
    
    exploration_content = f"""# GA4 Data Exploration Report
Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Dataset: {DATASET_ID}

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹ã¨SQLä½œæˆã«å¿…è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
"""
    
    # ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
    if explorations.get('data_summary'):
        summary = explorations['data_summary']
        exploration_content += f"""
- **æœŸé–“**: {summary.get('min_date', 'N/A')} ï½ {summary.get('max_date', 'N/A')}
- **ç·ã‚¤ãƒ™ãƒ³ãƒˆæ•°**: {summary.get('total_events', 'N/A'):,}
- **ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°**: {summary.get('total_users', 'N/A'):,}
"""
    
    # åˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆ
    exploration_content += f"""
## ğŸ¯ åˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆå (ä¸Šä½20å€‹)

å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆåã¨ãã®é »åº¦:
"""
    for event in explorations.get('available_events', []):
        exploration_content += f"""
- **{event['event_name']}**: {event['event_count']:,}å› ({event['unique_users']:,}ãƒ¦ãƒ¼ã‚¶ãƒ¼)"""
    
    # ãƒ‡ãƒã‚¤ã‚¹ã‚«ãƒ†ã‚´ãƒª
    exploration_content += f"""

## ğŸ“± åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ã‚«ãƒ†ã‚´ãƒª

device.category ã®å®Ÿéš›ã®å€¤:
"""
    for device in explorations.get('device_categories', []):
        exploration_content += f"""
- **{device['device_category']}**: {device['event_count']:,}å› ({device['unique_users']:,}ãƒ¦ãƒ¼ã‚¶ãƒ¼)"""
    
    # æµå…¥å…ƒ
    exploration_content += f"""

## ğŸŒ ä¸»è¦ãªæµå…¥å…ƒ

traffic_source ã®å®Ÿéš›ã®å€¤:
"""
    for source in explorations.get('traffic_sources', []):
        exploration_content += f"""
- source: **{source['source']}**, medium: **{source['medium']}** ({source['event_count']:,}å›)"""
    
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    exploration_content += f"""

## ğŸ”§ ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹event_params

å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚­ãƒ¼:
"""
    for param in explorations.get('common_event_params', []):
        exploration_content += f"""
- **{param['param_key']}**: {param['usage_count']:,}å›ä½¿ç”¨"""
    
    # åœ°ç†æƒ…å ±
    exploration_content += f"""

## ğŸŒ ä¸»è¦ãªå›½ãƒ»åœ°åŸŸ

geo.country ã®å®Ÿéš›ã®å€¤:
"""
    for country in explorations.get('top_countries', []):
        exploration_content += f"""
- **{country['country']}**: {country['event_count']:,}å›"""
    
    exploration_content += f"""

## ğŸ’¡ SQLä½œæˆæ™‚ã®é‡è¦ãªæŒ‡é‡

1. **ã‚¤ãƒ™ãƒ³ãƒˆå**: ä¸Šè¨˜ã®ã€Œåˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆåã€ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„
2. **ãƒ‡ãƒã‚¤ã‚¹ã‚«ãƒ†ã‚´ãƒª**: {', '.join([d['device_category'] for d in explorations.get('device_categories', [])])}
3. **æ—¥ä»˜ç¯„å›²**: {explorations.get('data_summary', {}).get('min_date', '20201101')} ï½ {explorations.get('data_summary', {}).get('max_date', '20210131')}
4. **ãƒ†ãƒ¼ãƒ–ãƒ«å**: `{source_project_id}.{dataset_name}.events_*`
5. **å¿…é ˆWHEREå¥**: `WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'`

## SQLä¾‹

```sql
-- åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªä¾‹
SELECT 
    event_name,
    COUNT(*) as event_count,
    COUNT(DISTINCT user_pseudo_id) as unique_users
FROM `{source_project_id}.{dataset_name}.events_*`
WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20210131'
GROUP BY event_name
ORDER BY event_count DESC
```
"""
    
    return exploration_content

def get_field_description(field_name):
    """ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«åŸºã¥ãèª¬æ˜ã‚’è¿”ã™"""
    descriptions = {
        'event_date': 'ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿæ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰',
        'event_timestamp': 'ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ï¼‰',
        'event_name': 'ã‚¤ãƒ™ãƒ³ãƒˆåï¼ˆpage_view, purchaseç­‰ï¼‰',
        'event_value_in_usd': 'ã‚¤ãƒ™ãƒ³ãƒˆä¾¡å€¤ï¼ˆç±³ãƒ‰ãƒ«ï¼‰',
        'user_id': 'ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ID',
        'user_pseudo_id': 'åŒ¿åãƒ¦ãƒ¼ã‚¶ãƒ¼è­˜åˆ¥å­',
        'user_first_touch_timestamp': 'åˆå›æ¥è§¦ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—',
        'event_params': 'ã‚¤ãƒ™ãƒ³ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆkey-valueæ§‹é€ ï¼‰',
        'user_properties': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼å±æ€§æƒ…å ±',
        'device': 'ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ï¼ˆã‚«ãƒ†ã‚´ãƒªã€OSç­‰ï¼‰',
        'geo': 'åœ°ç†æƒ…å ±ï¼ˆå›½ã€åœ°åŸŸç­‰ï¼‰',
        'traffic_source': 'æµå…¥å…ƒæƒ…å ±ï¼ˆsource, mediumç­‰ï¼‰',
        'app_info': 'ã‚¢ãƒ—ãƒªæƒ…å ±',
        'user_ltv': 'ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ãƒãƒªãƒ¥ãƒ¼',
        'privacy_info': 'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼åŒæ„æƒ…å ±'
    }
    return descriptions.get(field_name, 'GA4æ¨™æº–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰')

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã®å–å¾—
    tables = list(client.list_tables(dataset_ref))
    
    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’å–å¾—
    rows = []
    for table_item in tables:
        table_id = table_item.table_id
        table_ref = dataset_ref.table(table_id)
        table = client.get_table(table_ref)
        for field in table.schema:
            rows.append({
                "Table": table_id,
                "Field": field.name,
                "Type": field.field_type,
                "Mode": field.mode
            })
    
    # DataFrame ã«æ•´å½¢
    schema_df = pd.DataFrame(rows)
    print(f"ğŸ“‹ åŸºæœ¬ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±: {len(schema_df)}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰")
    
    # CSVã«ä¿å­˜
    schema_df.to_csv(SCHEMA_CSV_FILE, index=False)
    
    # ãƒ‡ãƒ¼ã‚¿å†…å®¹ã‚’æ¢ç´¢
    explorations = explore_data_content()
    
    # ç´”ç²‹ãªã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    schema_content = generate_schema_file(schema_df)
    with open(SCHEMA_TXT_FILE, 'w', encoding='utf-8') as f:
        f.write(schema_content)
    
    # ãƒ‡ãƒ¼ã‚¿æ¢ç´¢çµæœã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ç”Ÿæˆ
    exploration_content = generate_data_exploration_file(explorations)
    with open(DATA_EXPLORATION_FILE, 'w', encoding='utf-8') as f:
        f.write(exploration_content)
    
    print(f"\nâœ… ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ: {SCHEMA_TXT_FILE}")
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ: {DATA_EXPLORATION_FILE}")
    print(f"âœ… CSVã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ: {SCHEMA_CSV_FILE}")

if __name__ == "__main__":
    main()
